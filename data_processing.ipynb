{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1f7843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.warp import reproject\n",
    "\n",
    "# ===== USER SETTINGS =====\n",
    "SCENE_DIR = \"SR_outputs\"\n",
    "OUT_DIR = os.path.join(SCENE_DIR, \"pansharpened_Brovey_basic\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "MS_BANDS = [1, 2, 3, 4, 5, 7]  # Multispectral bands\n",
    "PAN_BAND = 8                   # Panchromatic band\n",
    "\n",
    "# ===== HELPER FUNCTION =====\n",
    "def find_band(scene_dir, band_id):\n",
    "    for f in os.listdir(scene_dir):\n",
    "        if f.lower().endswith(f\"_b{band_id}.tif\") or f.lower().endswith(f\"_b{band_id}.tiff\"):\n",
    "            return os.path.join(scene_dir, f)\n",
    "    raise FileNotFoundError(f\"Band {band_id} not found in {scene_dir}\")\n",
    "\n",
    "# ===== LOAD PAN =====\n",
    "pan_path = find_band(SCENE_DIR, PAN_BAND)\n",
    "with rasterio.open(pan_path) as pan_ds:\n",
    "    pan = pan_ds.read(1).astype(np.float32)\n",
    "    pan_meta = pan_ds.meta.copy()\n",
    "\n",
    "# ===== REPROJECT MS BANDS =====\n",
    "ms_list = []\n",
    "for b in MS_BANDS:\n",
    "    ms_path = find_band(SCENE_DIR, b)\n",
    "    with rasterio.open(ms_path) as ms_ds:\n",
    "        ms_up = np.empty_like(pan, dtype=np.float32)\n",
    "        reproject(\n",
    "            source=rasterio.band(ms_ds, 1),\n",
    "            destination=ms_up,\n",
    "            src_transform=ms_ds.transform,\n",
    "            src_crs=ms_ds.crs,\n",
    "            dst_transform=pan_meta[\"transform\"],\n",
    "            dst_crs=pan_meta[\"crs\"],\n",
    "            resampling=Resampling.bilinear\n",
    "        )\n",
    "        ms_list.append(ms_up)\n",
    "\n",
    "stack = np.stack(ms_list, axis=0)  # shape: (bands, H, W)\n",
    "stack = stack / (np.nanpercentile(stack, 99.9, axis=(1,2), keepdims=True) + 1e-6)\n",
    "\n",
    "# ===== BROVEY TRANSFORMATION =====\n",
    "denominator = np.sum(stack, axis=0) + 1e-6  # avoid divide by zero\n",
    "brovey_stack = stack * (pan / denominator)\n",
    "\n",
    "# ===== SAVE OUTPUTS =====\n",
    "for i, b in enumerate(MS_BANDS):\n",
    "    out_path = os.path.join(OUT_DIR, f\"LE09_2025_B{b:02d}_BROVEY.TIF\")\n",
    "    meta = pan_meta.copy()\n",
    "    meta.update(count=1, dtype=\"float32\", nodata=0)\n",
    "    with rasterio.open(out_path, \"w\", **meta) as dst:\n",
    "        dst.write(np.clip(brovey_stack[i], 0, np.nanmax(brovey_stack[i])).astype(np.float32), 1)\n",
    "    print(f\"‚úÖ Band {b} saved ‚Üí {out_path}\")\n",
    "\n",
    "print(\"\\nüéØ Simple Brovey pansharpening complete.\")\n",
    "print(f\"Outputs saved to: {OUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c53a5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" create composite of SR bands 1-7 (1999-2025) \"\"\"\n",
    "\n",
    "import os\n",
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "# === Input directory containing SR bands ===\n",
    "SR_DIR = \"SR_outputs/pansharpened_Brovey_basic\"\n",
    "OUT_PATH = os.path.join(SR_DIR, \"LE09_SR_2025_COMPOSITE_B1toB7.tif\")\n",
    "\n",
    "bands = [1, 2, 3, 4, 5, 7]  # Landsat 7 SR bands\n",
    "\n",
    "# Load first band for metadata\n",
    "with rasterio.open(os.path.join(SR_DIR, f\"LE09_2025_B01_BROVEY.TIF\")) as src:\n",
    "    meta = src.meta.copy()\n",
    "    height, width = src.height, src.width\n",
    "\n",
    "# Create empty composite array\n",
    "stack = np.zeros((len(bands), height, width), dtype=\"float32\")\n",
    "\n",
    "# Load each band\n",
    "for i, b in enumerate(bands):\n",
    "    band_path = os.path.join(SR_DIR, f\"LE09_2025_B0{b}_BROVEY.TIF\")\n",
    "    with rasterio.open(band_path) as src:\n",
    "        stack[i] = src.read(1)\n",
    "\n",
    "# Update metadata\n",
    "meta.update({\n",
    "    \"count\": len(bands),\n",
    "    \"dtype\": \"float32\",\n",
    "    \"nodata\": 0\n",
    "})\n",
    "\n",
    "# Save composite\n",
    "with rasterio.open(OUT_PATH, \"w\", **meta) as dst:\n",
    "    dst.write(stack)\n",
    "    dst.descriptions = [f\"B{b}\" for b in bands]\n",
    "\n",
    "print(f\"‚úÖ Composite saved to:\\n  {OUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7291595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Compute MNDWI and create binary water mask with adaptive thresholding and morphological cleanup\"\"\"\n",
    "\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage.filters import threshold_otsu\n",
    "from scipy.ndimage import binary_opening, binary_closing\n",
    "\n",
    "INPUT_RASTER = \".tif\"\n",
    "OUT_MNDWI = INPUT_RASTER.replace(\".tif\", \"_MNDWI.tif\")\n",
    "OUT_BIN = INPUT_RASTER.replace(\".tif\", \"_MNDWI_BIN.tif\")\n",
    "\n",
    "# --- Load Green and SWIR1 ---\n",
    "with rasterio.open(INPUT_RASTER) as src:\n",
    "    green = src.read(2).astype(\"float32\")  # Band 2 = Green\n",
    "    swir1 = src.read(5).astype(\"float32\")  # Band 5 = SWIR1\n",
    "    meta = src.meta.copy()\n",
    "\n",
    "# --- Compute MNDWI ---\n",
    "mndwi = (green - swir1) / (green + swir1 + 1e-6)\n",
    "mndwi = np.clip(mndwi, -1, 1)\n",
    "\n",
    "# --- Adaptive Threshold (Otsu or Mean+Std) ---\n",
    "valid = np.isfinite(mndwi)\n",
    "try:\n",
    "    t = threshold_otsu(mndwi[valid])\n",
    "    print(f\"üîπ Using Otsu threshold: {t:.3f}\")\n",
    "except Exception:\n",
    "    t = np.nanmean(mndwi[valid]) + 0.5 * np.nanstd(mndwi[valid])\n",
    "    print(f\"üîπ Using adaptive mean+std threshold: {t:.3f}\")\n",
    "\n",
    "# --- Binary Mask ---\n",
    "binary = np.zeros_like(mndwi, dtype=\"uint8\")\n",
    "binary[mndwi > t] = 1\n",
    "\n",
    "# --- Morphological cleanup ---\n",
    "binary = binary_closing(binary, structure=np.ones((3,3)))\n",
    "binary = binary_opening(binary, structure=np.ones((3,3)))\n",
    "\n",
    "# --- Save MNDWI and mask ---\n",
    "meta.update(count=1, dtype=\"float32\", nodata=np.nan)\n",
    "with rasterio.open(OUT_MNDWI, \"w\", **meta) as dst:\n",
    "    dst.write(mndwi, 1)\n",
    "\n",
    "meta.update(dtype=\"uint8\", nodata=0)\n",
    "with rasterio.open(OUT_BIN, \"w\", **meta) as dst:\n",
    "    dst.write(binary, 1)\n",
    "\n",
    "print(f\"‚úÖ MNDWI and improved binary mask saved:\\n  {OUT_MNDWI}\\n  {OUT_BIN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29beebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Prepare shoreline shapefile for DSAS analysis by adding required fields and saving as GeoJSON \"\"\"\n",
    "\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# === USER INPUT ===\n",
    "input_shp = \".shp\"   # your shapefile for shoreline\n",
    "output_dir = \"replace with output path\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === READ SHAPEFILE ===\n",
    "gdf = gpd.read_file(input_shp)\n",
    "\n",
    "# === EXTRACT YEAR FROM FILENAME ===\n",
    "basename = os.path.basename(input_shp)\n",
    "name, _ = os.path.splitext(basename)\n",
    "m = re.search(r\"(\\d{4})\", name)\n",
    "if m:\n",
    "    year = m.group(1)\n",
    "    date_value = f\"{year}-09-13\"  # pick mid-year or your acquisition date\n",
    "else:\n",
    "    date_value = \"1900-01-01\"\n",
    "\n",
    "# === ADD REQUIRED DSAS FIELDS ===\n",
    "gdf[\"Date_\"] = date_value\n",
    "gdf[\"ID\"] = range(1, len(gdf) + 1)\n",
    "gdf[\"ShoreType\"] = \"Derived\"\n",
    "gdf[\"UNCERTAINTY\"] = 7.5  # leave 0 if no uncertainty available\n",
    "gdf[\"Group\"] = \"Wolin_Usedom\"\n",
    "\n",
    "# === SAVE AS GEOJSON ===\n",
    "out_geojson = os.path.join(output_dir, name + \".geojson\")\n",
    "gdf.to_file(out_geojson, driver=\"GeoJSON\")\n",
    "\n",
    "print(f\"‚úÖ Converted: {input_shp}\")\n",
    "print(f\"‚Üí Saved DSAS-ready GeoJSON: {out_geojson}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7b41bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Merge multiple shoreline GeoJSON files into a single GeoJSON for DSAS\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# === USER INPUT ===\n",
    "geojson_dir = \"\"\n",
    "output_merged = os.path.join(geojson_dir, \".geojson\")\n",
    "\n",
    "# === FIND ALL GEOJSON FILES ===\n",
    "geojson_files = glob.glob(os.path.join(geojson_dir, \"*.geojson\"))\n",
    "if not geojson_files:\n",
    "    raise FileNotFoundError(\"No GeoJSON files found in the folder.\")\n",
    "\n",
    "# === MERGE ===\n",
    "gdf_list = [gpd.read_file(f) for f in geojson_files]\n",
    "merged = gpd.GeoDataFrame(pd.concat(gdf_list, ignore_index=True), crs=gdf_list[0].crs)\n",
    "\n",
    "# === SAVE MERGED FILE ===\n",
    "merged.to_file(output_merged, driver=\"GeoJSON\")\n",
    "\n",
    "print(f\"‚úÖ Merged {len(geojson_files)} GeoJSON files\")\n",
    "print(f\"‚Üí Saved combined shoreline dataset: {output_merged}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5399d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Processing and compiling dataset into a single framework that is ready for inputting into the model_training_inference.ipynb script.\n",
    "\n",
    "The user needs to check example_data.csv uploaded on github to know how the final dataset will look like and modify the model_training_inference.ipynb depending on the dataset\n",
    "\n",
    "This script:\n",
    "‚úÖ Extracts NSM from DSAS GeoJSONs (sector-I, sector-II, sector-III)\n",
    "‚úÖ Merges with ERA5 wave height and direction data\n",
    "‚úÖ Merges with SLR data\n",
    "‚úÖ Adds coordinates and bearings for transects\n",
    "‚úÖ Outputs one clean CSV file ready for model input\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pyproj import Transformer, Geod\n",
    "\n",
    "# ======================================================\n",
    "# USER INPUTS\n",
    "# ======================================================\n",
    "\n",
    "# Input DSAS folders for each sector\n",
    "SECTOR_PATHS = {\n",
    "    \"sector-I\": \"\",\n",
    "    \"sector-II\": \"\",\n",
    "    \"sector-III\": \"\"\n",
    "}\n",
    "\n",
    "# Output file paths\n",
    "OUT_CSV = \"Table_for_DL.csv\"\n",
    "FINAL_SORTED_CSV = \"Table_for_DL_sorted.csv\"\n",
    "FINAL_COORDS_CSV = \"Table_for_DL_with_coords.csv\"\n",
    "\n",
    "# ERA5 base data folder (wave height + direction)\n",
    "ERA5_FOLDERS = {\n",
    "    \"sector-I\": \"usedom_ERA5_data\",\n",
    "    \"sector-II\": \"Wolin_ERA5_data\",\n",
    "    \"sector-III\": \"miedzywodzie_ERA5_data\"\n",
    "}\n",
    "\n",
    "# Sea-level rise data\n",
    "SLR_CSV = \"slr_change_mm_intervals.csv\"\n",
    "\n",
    "# CRS transformation (adjust if your DSAS uses a different UTM zone)\n",
    "SOURCE_CRS = \"EPSG:32633\"\n",
    "TARGET_CRS = \"EPSG:4326\"\n",
    "\n",
    "# ======================================================\n",
    "# STEP 1 ‚Äî Extract NSM from DSAS GeoJSONs (all sectors)\n",
    "# ======================================================\n",
    "\n",
    "all_records = []\n",
    "\n",
    "for region_name, dsas_dir in SECTOR_PATHS.items():\n",
    "    geojson_files = sorted(glob.glob(os.path.join(dsas_dir, \"*.geojson\")))\n",
    "    if not geojson_files:\n",
    "        print(f\"‚ö†Ô∏è No GeoJSON files found for {region_name}\")\n",
    "        continue\n",
    "\n",
    "    for geojson_file in geojson_files:\n",
    "        try:\n",
    "            gdf = gpd.read_file(geojson_file)\n",
    "\n",
    "            transect_col = next((c for c in gdf.columns if \"transect\" in c.lower()), None)\n",
    "            nsm_col = next((c for c in gdf.columns if \"nsm\" in c.lower()), None)\n",
    "            if not transect_col or not nsm_col:\n",
    "                print(f\"‚ö†Ô∏è Skipping {geojson_file} ‚Äî missing required columns.\")\n",
    "                continue\n",
    "\n",
    "            # Extract start/end years from filename (e.g. 1987_1992_rates.geojson)\n",
    "            base = os.path.basename(geojson_file)\n",
    "            years = [int(s) for s in base.split(\"_\") if s.isdigit()]\n",
    "            if len(years) < 2:\n",
    "                print(f\"‚ö†Ô∏è Skipping {base}: could not extract start/end years.\")\n",
    "                continue\n",
    "\n",
    "            start_year, end_year = years[:2]\n",
    "            duration_yrs = end_year - start_year\n",
    "\n",
    "            df = pd.DataFrame({\n",
    "                \"region\": region_name,\n",
    "                \"transect_id\": gdf[transect_col],\n",
    "                \"start_date\": [f\"{start_year}-06-01\"] * len(gdf),\n",
    "                \"end_date\": [f\"{end_year}-06-01\"] * len(gdf),\n",
    "                \"duration_yrs\": [duration_yrs] * len(gdf),\n",
    "                \"nsm\": gdf[nsm_col]\n",
    "            })\n",
    "\n",
    "            all_records.append(df)\n",
    "            print(f\"‚úÖ Processed {base} ({len(df)} records) for {region_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error reading {geojson_file}: {e}\")\n",
    "\n",
    "# Combine all extracted NSM data\n",
    "final_df = pd.concat(all_records, ignore_index=True)\n",
    "final_df.to_csv(OUT_CSV, index=False)\n",
    "print(f\"‚úÖ NSM data combined for all sectors ‚Üí {OUT_CSV}\")\n",
    "\n",
    "# ======================================================\n",
    "# STEP 2 ‚Äî Merge ERA5 wave height + direction data\n",
    "# ======================================================\n",
    "\n",
    "nsm_df = final_df.copy()\n",
    "nsm_df[\"start_date\"] = pd.to_datetime(nsm_df[\"start_date\"])\n",
    "nsm_df[\"end_date\"] = pd.to_datetime(nsm_df[\"end_date\"])\n",
    "nsm_df[\"start_year\"] = nsm_df[\"start_date\"].dt.year\n",
    "nsm_df[\"end_year\"] = nsm_df[\"end_date\"].dt.year\n",
    "\n",
    "merged_all = []\n",
    "\n",
    "for region, folder in ERA5_FOLDERS.items():\n",
    "    wh_csv = os.path.join(folder, \"wave_height_intervals.csv\")\n",
    "    wd_csv = os.path.join(folder, \"wave_direction_intervals.csv\")\n",
    "\n",
    "    subset = nsm_df[nsm_df[\"region\"] == region].copy()\n",
    "    if not (os.path.exists(wh_csv) and os.path.exists(wd_csv)):\n",
    "        print(f\"‚ö†Ô∏è Skipping {region} (missing ERA5 files)\")\n",
    "        merged_all.append(subset)\n",
    "        continue\n",
    "\n",
    "    wh_df = pd.read_csv(wh_csv)\n",
    "    wd_df = pd.read_csv(wd_csv)\n",
    "\n",
    "    wh_df = wh_df.rename(columns={\n",
    "        \"mean\": \"swh_mean\", \"min\": \"swh_min\", \"max\": \"swh_max\",\n",
    "        \"std\": \"swh_std\", \"p10\": \"swh_p10\", \"p50\": \"swh_p50\", \"p90\": \"swh_p90\"\n",
    "    })\n",
    "\n",
    "    for df in [wh_df, wd_df]:\n",
    "        df[\"start_year\"] = df[\"start_year\"].astype(int)\n",
    "        df[\"end_year\"] = df[\"end_year\"].astype(int)\n",
    "\n",
    "    merged = pd.merge(subset, wh_df, on=[\"start_year\", \"end_year\"], how=\"left\")\n",
    "    overlapping = [c for c in wd_df.columns if c in merged.columns and c not in [\"start_year\", \"end_year\"]]\n",
    "    wd_df = wd_df.drop(columns=overlapping)\n",
    "    merged = pd.merge(merged, wd_df, on=[\"start_year\", \"end_year\"], how=\"left\")\n",
    "\n",
    "    merged_all.append(merged)\n",
    "    print(f\"‚úÖ ERA5 merged for {region} ‚Üí {len(merged)} records\")\n",
    "\n",
    "wave_merged_df = pd.concat(merged_all, ignore_index=True)\n",
    "wave_merged_df.to_csv(OUT_CSV, index=False)\n",
    "print(f\"‚úÖ ERA5 data merged into main table ‚Üí {OUT_CSV}\")\n",
    "\n",
    "# ======================================================\n",
    "# STEP 3 ‚Äî Merge Sea Level Rise (SLR) data\n",
    "# ======================================================\n",
    "\n",
    "# ======================================================\n",
    "# STEP 3 ‚Äî Extract Transect-Specific SLR from CMIP6 .nc files\n",
    "# ======================================================\n",
    "\n",
    "import xarray as xr\n",
    "from scipy.spatial import cKDTree\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def extract_slr_nc_files_station_based(nc_dir, transect_df):\n",
    "    all_slr = []\n",
    "    files = sorted(glob.glob(os.path.join(nc_dir, \"*.nc\")))\n",
    "\n",
    "    for f in files:\n",
    "        print(f\"\\nProcessing SLR file: {os.path.basename(f)}\")\n",
    "        ds = xr.open_dataset(f)\n",
    "        var_name = list(ds.data_vars)[0]  # e.g., 'MSL'\n",
    "        print(f\"Variable in dataset: {var_name}\")\n",
    "\n",
    "        # Station coordinates\n",
    "        station_x = ds[\"station_x_coordinate\"].values\n",
    "        station_y = ds[\"station_y_coordinate\"].values\n",
    "        coords = np.column_stack((station_x, station_y))\n",
    "        tree = cKDTree(coords)\n",
    "        print(f\"Stations shape: {coords.shape}\")\n",
    "\n",
    "        # Transect coordinates\n",
    "        tran_coords = np.column_stack((transect_df[\"lon\"].values, transect_df[\"lat\"].values))\n",
    "        print(f\"Transects shape: {tran_coords.shape}\")\n",
    "\n",
    "        # Query nearest station for each transect\n",
    "        _, idxs = tree.query(tran_coords)\n",
    "        print(f\"Idxs min/max: {idxs.min()}/{idxs.max()}\")\n",
    "\n",
    "        # Extract SLR values for those stations safely\n",
    "        slr_data = ds[var_name].values\n",
    "        print(f\"SLR data shape: {slr_data.shape}\")\n",
    "\n",
    "        if slr_data.ndim == 1:\n",
    "            if idxs.max() >= slr_data.shape[0]:\n",
    "                print(\"‚ö†Ô∏è idxs exceed slr_data.shape[0], clipping indices\")\n",
    "                idxs = np.clip(idxs, 0, slr_data.shape[0]-1)\n",
    "            slr_vals = slr_data[idxs]\n",
    "        elif slr_data.ndim == 2:\n",
    "            # stations x time\n",
    "            if slr_data.shape[1] == 1:\n",
    "                if idxs.max() >= slr_data.shape[0]:\n",
    "                    print(\"‚ö†Ô∏è idxs exceed slr_data.shape[0], clipping indices\")\n",
    "                    idxs = np.clip(idxs, 0, slr_data.shape[0]-1)\n",
    "                slr_vals = slr_data[idxs, 0]\n",
    "            else:\n",
    "                if idxs.max() >= slr_data.shape[0]:\n",
    "                    print(\"‚ö†Ô∏è idxs exceed slr_data.shape[0], clipping indices\")\n",
    "                    idxs = np.clip(idxs, 0, slr_data.shape[0]-1)\n",
    "                # pick first time step as representative\n",
    "                slr_vals = slr_data[idxs, 0]\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected shape for {var_name}: {slr_data.shape}\")\n",
    "\n",
    "        # Convert to mm\n",
    "        slr_vals_mm = slr_vals * 1000\n",
    "\n",
    "        # Build dataframe for this year\n",
    "        df_year = transect_df.copy()\n",
    "        df_year[\"slr_mm\"] = slr_vals_mm\n",
    "\n",
    "        # Extract year from filename (assumes pattern: *_YYYY_MSL_v1.nc)\n",
    "        fname = os.path.basename(f)\n",
    "        year_str = fname.split(\"_\")[-3]\n",
    "        df_year[\"end_year\"] = int(year_str)\n",
    "\n",
    "        all_slr.append(df_year)\n",
    "\n",
    "    return pd.concat(all_slr, ignore_index=True)\n",
    "\n",
    "# Run extraction for historical and future SLR\n",
    "slr_hist = extract_slr_nc_files_station_based(HIST_DIR, merged_coords_df)\n",
    "slr_future = extract_slr_nc_files_station_based(FUT_DIR, merged_coords_df)\n",
    "slr_df = pd.concat([slr_hist, slr_future], ignore_index=True)\n",
    "slr_df.to_csv(FINAL_COORDS_CSV, index=False)\n",
    "print(f\"‚úÖ SLR merged and final dataset saved ‚Üí {FINAL_COORDS_CSV}\")\n",
    "\n",
    "# ======================================================\n",
    "# STEP 4 ‚Äî Sort by region and transect_id (custom order)\n",
    "# ======================================================\n",
    "\n",
    "region_order = [\"sector-I\", \"sector-II\", \"sector-III\"]\n",
    "merged_df[\"region_order\"] = merged_df[\"region\"].apply(lambda x: region_order.index(x))\n",
    "sorted_df = merged_df.sort_values(by=[\"region_order\", \"transect_id\"]).drop(columns=\"region_order\")\n",
    "sorted_df.to_csv(FINAL_SORTED_CSV, index=False)\n",
    "print(f\"‚úÖ Sorted data saved ‚Üí {FINAL_SORTED_CSV}\")\n",
    "\n",
    "# ======================================================\n",
    "# STEP 5 ‚Äî Add coordinates + bearing (user‚Äôs original function)\n",
    "# ======================================================\n",
    "\n",
    "transformer = Transformer.from_crs(SOURCE_CRS, TARGET_CRS, always_xy=True)\n",
    "geod = Geod(ellps=\"WGS84\")\n",
    "\n",
    "def extract_coords_and_bearing(geojson_path, region_name):\n",
    "    with open(geojson_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    records = []\n",
    "    for feature in data[\"features\"]:\n",
    "        props = feature[\"properties\"]\n",
    "        tid = props.get(\"TransectId\") or props.get(\"transect_id\")\n",
    "        geom = feature[\"geometry\"]\n",
    "\n",
    "        if geom is None or \"coordinates\" not in geom:\n",
    "            continue\n",
    "        coords = geom[\"coordinates\"]\n",
    "        if not coords or len(coords) < 2:\n",
    "            continue\n",
    "\n",
    "        x1, y1 = coords[0]\n",
    "        x2, y2 = coords[-1]\n",
    "        lon1, lat1 = transformer.transform(x1, y1)\n",
    "        lon2, lat2 = transformer.transform(x2, y2)\n",
    "\n",
    "        az12, az21, dist = geod.inv(lon1, lat1, lon2, lat2)\n",
    "        bearing_deg = az12 % 360\n",
    "        lon_mid, lat_mid = (lon1 + lon2) / 2, (lat1 + lat2) / 2\n",
    "\n",
    "        records.append({\n",
    "            \"region\": region_name,\n",
    "            \"transect_id\": tid,\n",
    "            \"lon\": lon_mid,\n",
    "            \"lat\": lat_mid,\n",
    "            \"bearing_deg\": bearing_deg\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "coord_all = []\n",
    "for region, path in SECTOR_PATHS.items():\n",
    "    geojsons = glob.glob(os.path.join(path, \"*rates*.geojson\"))\n",
    "    if not geojsons:\n",
    "        print(f\"‚ö†Ô∏è No GeoJSON for {region}\")\n",
    "        continue\n",
    "    geojson_path = geojsons[0]\n",
    "    print(f\"üìÇ Extracting coordinates & bearings from {geojson_path}\")\n",
    "    region_coords = extract_coords_and_bearing(geojson_path, region)\n",
    "    coord_all.append(region_coords)\n",
    "\n",
    "coords_df = pd.concat(coord_all, ignore_index=True)\n",
    "merged_coords_df = sorted_df.merge(coords_df, on=[\"region\", \"transect_id\"], how=\"left\")\n",
    "merged_coords_df.to_csv(FINAL_COORDS_CSV, index=False)\n",
    "print(f\"‚úÖ Final dataset with coordinates saved ‚Üí {FINAL_COORDS_CSV}\")\n",
    "\n",
    "print(\"\\nüéØ All preprocessing complete ‚Äî dataset ready for model training.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
