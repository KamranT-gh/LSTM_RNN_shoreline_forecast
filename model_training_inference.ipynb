{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86480d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Seq1 Training + Recursive inference (single-step repeated) using LSTM encoder regressor.\n",
    "\n",
    "- Train 7 models using LOOKBACKS = [2,3,4,5,6,7,8]\n",
    "- For each lookback:\n",
    "      • Build windows\n",
    "      • Train model\n",
    "      • Compute validation R²\n",
    "- Choose best lookback by highest R² (tie → smallest lookback)\n",
    "- Save ONLY that model + scaler (under the normal filenames)\n",
    "- Run recursive inference ONLY AFTER best model is chosen\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ---------------- USER SETTINGS ----------------\n",
    "INPUT_CSV = \"Path to your CSV\"\n",
    "FUTURE_SLR_FILE = \"Path to your SLR data csv\"\n",
    "\n",
    "OUTDIR = \"Output directory file\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "TARGET = \"nsm\"\n",
    "TRAIN_END_YEAR = 2021 #year where you expect the model to end training\n",
    "VAL_YEAR = 2025\n",
    "\n",
    "LOOKBACKS = [2,3,4,5,6,7,8] \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-2\n",
    "EPOCHS = 500\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# FEATURES (can change based on what features your CSV contains)\n",
    "BASE_SEQUENCE_FEATURES = [\n",
    "    \"swh_max\", \"swh_mean\", \"swh_std\", \"swh_storm_ratio\",\n",
    "    \"attack_sin\", \"attack_cos\", \"slr_mm\"\n",
    "]\n",
    "INTERACTION_FEATURES = [\"slr_x_storm\", \"slr_x_sin\", \"slr_x_cos\"]\n",
    "SEQUENCE_FEATURES = BASE_SEQUENCE_FEATURES + INTERACTION_FEATURES\n",
    "\n",
    "FUTURE_YEARS = [2030, 2035, 2040, 2045, 2050]\n",
    "\n",
    "SCALER_PATH = os.path.join(OUTDIR, f\"scaler_{VAL_YEAR}.joblib\")\n",
    "MODEL_PATH = os.path.join(OUTDIR, f\"lstm_encoder_model_{VAL_YEAR}.pt\")\n",
    "\n",
    "# ---------------- 1. LOAD + FEATURE ENGINEER ----------------\n",
    "print(\"Loading dataset:\", INPUT_CSV)\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "df[\"swh_storm_ratio\"] = df[\"swh_max\"] / df[\"swh_mean\"]\n",
    "\n",
    "if all(c in df.columns for c in [\"wd_mean_deg\", \"bearing_deg\"]):\n",
    "    angle_diff = df[\"wd_mean_deg\"] - df[\"bearing_deg\"]\n",
    "    df[\"angle_of_attack\"] = np.abs(angle_diff % 360)\n",
    "    df[\"angle_of_attack\"] = np.minimum(df[\"angle_of_attack\"], 360 - df[\"angle_of_attack\"])\n",
    "    df[\"attack_sin\"] = np.sin(np.radians(df[\"angle_of_attack\"]))\n",
    "    df[\"attack_cos\"] = np.cos(np.radians(df[\"angle_of_attack\"]))\n",
    "else:\n",
    "    df[\"attack_sin\"] = df.get(\"attack_sin\", 0.0)\n",
    "    df[\"attack_cos\"] = df.get(\"attack_cos\", 0.0)\n",
    "\n",
    "df[\"slr_x_storm\"] = df[\"slr_mm\"] * df[\"swh_storm_ratio\"]\n",
    "df[\"slr_x_sin\"] = df[\"slr_mm\"] * df[\"attack_sin\"]\n",
    "df[\"slr_x_cos\"] = df[\"slr_mm\"] * df[\"attack_cos\"]\n",
    "\n",
    "train_df = df[df[\"end_year\"] <= TRAIN_END_YEAR].copy()\n",
    "val_df   = df[df[\"end_year\"] <= VAL_YEAR].copy()\n",
    "\n",
    "print(\"Train rows:\", len(train_df), \"Val rows:\", len(val_df))\n",
    "\n",
    "# ---------------- HELPER: build windows ----------------\n",
    "def build_single_step_windows(df_in, lookback, seq_features):\n",
    "    X_list, y_list, meta_list = [], [], []\n",
    "    for tid, g in df_in.groupby(\"transect_id\"):\n",
    "        g = g.sort_values(\"end_year\").reset_index(drop=True)\n",
    "        seq = g[seq_features].values\n",
    "        n = len(g)\n",
    "        if n <= lookback: continue\n",
    "        for i in range(lookback, n):\n",
    "            X_list.append(seq[i - lookback:i])\n",
    "            y_list.append(g.loc[i, TARGET])\n",
    "            meta_list.append(g.iloc[i].to_dict())\n",
    "    if len(X_list) == 0:\n",
    "        return np.empty((0, lookback, len(seq_features))), np.empty((0,)), pd.DataFrame()\n",
    "    return np.stack(X_list), np.array(y_list), pd.DataFrame(meta_list)\n",
    "\n",
    "# ---------------- MODEL CLASS ----------------\n",
    "class LSTMEncoderRegressor(nn.Module):\n",
    "    def __init__(self, n_features, hidden_size=64, n_layers=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_features,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        h_last = out[:, -1]\n",
    "        return self.fc(h_last).squeeze(-1)\n",
    "\n",
    "# ---------------- 2A. TRAIN OVER ALL LOOKBACKS ----------------\n",
    "results = []  # store: (lookback, r2, model_state_dict, scaler_obj, X_val_data)\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"TRAINING OVER LOOKBACKS: 2–8\")\n",
    "print(\"==============================\")\n",
    "\n",
    "for LOOKBACK in LOOKBACKS:\n",
    "    print(f\"\\n---- Training for LOOKBACK = {LOOKBACK} ----\")\n",
    "\n",
    "    # build windows\n",
    "    X_train, Y_train, _ = build_single_step_windows(train_df, LOOKBACK, SEQUENCE_FEATURES)\n",
    "    X_val,   Y_val,   _ = build_single_step_windows(val_df, LOOKBACK, SEQUENCE_FEATURES)\n",
    "\n",
    "    if X_train.shape[0] == 0:\n",
    "        print(f\"Skipping LOOKBACK {LOOKBACK} — insufficient historical rows\")\n",
    "        continue\n",
    "\n",
    "    # scale\n",
    "    n_features = X_train.shape[2]\n",
    "    scaler = StandardScaler()\n",
    "    flat_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    flat_val   = X_val.reshape(X_val.shape[0], -1)\n",
    "\n",
    "    flat_train_scaled = scaler.fit_transform(flat_train)\n",
    "    flat_val_scaled   = scaler.transform(flat_val)\n",
    "\n",
    "    X_train_scaled = flat_train_scaled.reshape(X_train.shape)\n",
    "    X_val_scaled   = flat_val_scaled.reshape(X_val.shape)\n",
    "\n",
    "    # loaders\n",
    "    train_ds = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_train_scaled, dtype=torch.float32),\n",
    "        torch.tensor(Y_train, dtype=torch.float32)\n",
    "    )\n",
    "    val_ds = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X_val_scaled, dtype=torch.float32),\n",
    "        torch.tensor(Y_val, dtype=torch.float32)\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # model\n",
    "    model = LSTMEncoderRegressor(n_features).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # training\n",
    "    best_state = None\n",
    "    best_loss = np.inf\n",
    "    patience = 12\n",
    "    pat = 0\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            p = model(xb)\n",
    "            loss = criterion(p, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        # validate\n",
    "        model.eval()\n",
    "        vlosses = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                vlosses.append(criterion(model(xb), yb).item())\n",
    "        vloss = np.mean(vlosses)\n",
    "\n",
    "        # early stop\n",
    "        if vloss < best_loss:\n",
    "            best_loss = vloss\n",
    "            best_state = model.state_dict()\n",
    "            pat = 0\n",
    "        else:\n",
    "            pat += 1\n",
    "        if pat >= patience:\n",
    "            break\n",
    "\n",
    "    # compute final metrics\n",
    "    model.load_state_dict(best_state)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(torch.tensor(X_val_scaled, dtype=torch.float32).to(DEVICE)).cpu().numpy()\n",
    "    r2 = r2_score(Y_val, preds)\n",
    "    mae = mean_absolute_error(Y_val, preds)\n",
    "    rmse = np.sqrt(mean_squared_error(Y_val, preds))\n",
    "\n",
    "    print(f\"LOOKBACK {LOOKBACK}: R2 = {r2:.4f}, MAE = {mae:.4f}, RMSE = {rmse:.4f}\")\n",
    "\n",
    "    results.append((LOOKBACK, r2, mae, rmse, best_state, scaler, X_val_scaled, Y_val))\n",
    "    # ---------------- PART B: select best lookback, save model+scaler, THEN run inference ----------------\n",
    "\n",
    "if not results:\n",
    "    raise RuntimeError(\"No trained results found. Ensure Part A training loop ran and filled `results`.\")\n",
    "\n",
    "# Select best by highest R2, tie-breaker = smallest lookback\n",
    "sorted_results = sorted(results, key=lambda t: (-t[1], t[0]))  # sort by r2 desc, lookback asc\n",
    "best_lookback, best_r2, best_mae, best_rmse, best_state, best_scaler, best_Xval_scaled, best_Yval = sorted_results[0]\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(f\"BEST LOOKBACK SELECTED: {best_lookback}  (R2 = {best_r2:.4f}, MAE = {best_mae:.4f}, RMSE = {best_rmse:.4f})\")\n",
    "print(\"==============================\\n\")\n",
    "\n",
    "\n",
    "# Save best scaler and model state\n",
    "joblib.dump(best_scaler, SCALER_PATH)\n",
    "print(\"Saved scaler ->\", SCALER_PATH)\n",
    "\n",
    "# instantiate model with the number of features matching chosen lookback windows\n",
    "n_features = len(SEQUENCE_FEATURES)\n",
    "best_model = LSTMEncoderRegressor(n_features=n_features).to(DEVICE)\n",
    "best_model.load_state_dict(best_state)\n",
    "torch.save(best_model.state_dict(), MODEL_PATH)\n",
    "print(\"Saved model ->\", MODEL_PATH)\n",
    "\n",
    "# Rebuild validation windows for the best lookback to get proper validation data\n",
    "X_train_best, Y_train_best, _ = build_single_step_windows(train_df, best_lookback, SEQUENCE_FEATURES)\n",
    "X_val_best,   Y_val_best,   meta_val_best = build_single_step_windows(val_df, best_lookback, SEQUENCE_FEATURES)\n",
    "\n",
    "if len(X_val_best) > 0:\n",
    "    # Scale using the best scaler\n",
    "    n_features = X_val_best.shape[2]\n",
    "    flat_val_best = X_val_best.reshape(X_val_best.shape[0], -1)\n",
    "    flat_val_best_scaled = best_scaler.transform(flat_val_best)\n",
    "    X_val_best_scaled = flat_val_best_scaled.reshape(X_val_best.shape)\n",
    "    \n",
    "    # Get predictions using the best model\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_val_best = best_model(torch.tensor(X_val_best_scaled, dtype=torch.float32).to(DEVICE)).cpu().numpy()\n",
    "    \n",
    "    # Create validation export DataFrame\n",
    "    val_export = pd.DataFrame({\n",
    "        \"transect_id\": [m[\"transect_id\"] for m in meta_val_best.to_dict(\"records\")],\n",
    "        \"observed_nsm\": Y_val_best,\n",
    "        \"predicted_nsm_RNN\": y_pred_val_best,\n",
    "        \"region\": [m[\"region\"] for m in meta_val_best.to_dict(\"records\")],\n",
    "        \"bearing_deg\": [m[\"bearing_deg\"] for m in meta_val_best.to_dict(\"records\")]\n",
    "    })\n",
    "    \n",
    "    # Calculate metrics\n",
    "    val_mae = mean_absolute_error(Y_val_best, y_pred_val_best)\n",
    "    val_rmse = np.sqrt(mean_squared_error(Y_val_best, y_pred_val_best))\n",
    "    val_r2 = r2_score(Y_val_best, y_pred_val_best)\n",
    "    \n",
    "    print(f\"\\nValidation MAE: {val_mae:.4f}, RMSE: {val_rmse:.4f}, R2: {val_r2:.4f}\")\n",
    "    \n",
    "    # Save validation predictions\n",
    "    val_export.to_csv(os.path.join(OUTDIR, \"validation_predictions_2025.csv\"), index=False)\n",
    "    print(f\"Saved validation predictions -> {os.path.join(OUTDIR, 'validation_predictions_2025.csv')}\")\n",
    "    print(f\"Validation dataset shape: {val_export.shape}\")\n",
    "    print(f\"Sample validation \\n{val_export.head()}\")\n",
    "\n",
    "# --------------- Quick sanity metrics on best model (val R2/MAE) ---------------\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_val = best_model(torch.tensor(best_Xval_scaled, dtype=torch.float32).to(DEVICE)).cpu().numpy()\n",
    "val_mae = mean_absolute_error(best_Yval, y_pred_val)\n",
    "val_rmse = np.sqrt(mean_squared_error(best_Yval, y_pred_val))\n",
    "val_r2  = r2_score(best_Yval, y_pred_val)\n",
    "print(f\"\\nBest model validation MAE: {val_mae:.4f}, RMSE: {val_rmse:.4f}, R2: {val_r2:.4f}\")\n",
    "\n",
    "# Extract and save 2021 baseline shoreline points\n",
    "baseline_2021 = df[df[\"end_year\"] == 2021].copy()\n",
    "available_columns = [col for col in [\"region\", \"transect_id\", \"bearing_deg\", \"nsm\"] if col in baseline_2021.columns]\n",
    "baseline_2021_export = baseline_2021[available_columns].copy()\n",
    "if \"nsm\" in baseline_2021_export.columns:\n",
    "    baseline_2021_export = baseline_2021_export.rename(columns={\"nsm\": \"nsm_2021_baseline\"})\n",
    "baseline_2021_export.to_csv(os.path.join(OUTDIR, \"baseline_2021_shoreline.csv\"), index=False)\n",
    "print(f\"Saved 2021 baseline shoreline -> {os.path.join(OUTDIR, 'baseline_2021_shoreline.csv')}\")\n",
    "print(f\"Baseline 2021 dataset shape: {baseline_2021_export.shape}\")\n",
    "print(f\"Sample baseline \\n{baseline_2021_export.head()}\")\n",
    "\n",
    "# ---------------- 7. RECURSIVE INFERENCE (single-step repeated) ----------------\n",
    "print(\"\\nLoading transect-specific future SLR:\", FUTURE_SLR_FILE)\n",
    "future_slr = pd.read_csv(FUTURE_SLR_FILE)\n",
    "if \"year\" in future_slr.columns and \"end_year\" not in future_slr.columns:\n",
    "    future_slr = future_slr.rename(columns={\"year\": \"end_year\"})\n",
    "required = [\"transect_id\", \"end_year\", \"slr_mm\"]\n",
    "if not all(c in future_slr.columns for c in required):\n",
    "    raise RuntimeError(f\"FUTURE_SLR_FILE must contain columns: {required}\")\n",
    "\n",
    "slr_lookup = future_slr.set_index([\"transect_id\", \"end_year\"])[\"slr_mm\"]\n",
    "\n",
    "# Prepare historical last-known rows (up to 2025)\n",
    "hist_last = df[df[\"end_year\"] <= 2025].copy()\n",
    "\n",
    "# Helper: build initial window for selected best_lookback\n",
    "def build_initial_window_for_lookback(transect_id, lookback):\n",
    "    g = hist_last[hist_last[\"transect_id\"] == transect_id].sort_values(\"end_year\")\n",
    "    if len(g) < lookback:\n",
    "        return None\n",
    "    window = g.iloc[-lookback:][SEQUENCE_FEATURES].values.copy()\n",
    "    meta_row = g.iloc[-1]\n",
    "    return window, meta_row\n",
    "\n",
    "# scale helper using saved scaler\n",
    "def scale_window_with_saved_scaler(window):\n",
    "    flat = window.reshape(1, -1)\n",
    "    flat_scaled = best_scaler.transform(flat)\n",
    "    return flat_scaled.reshape(1, best_lookback, n_features)\n",
    "\n",
    "# Setup outputs container\n",
    "outputs_per_year = {y: [] for y in FUTURE_YEARS}\n",
    "transect_ids = sorted(hist_last[\"transect_id\"].unique())\n",
    "print(f\"\\nStarting recursive inference for {len(transect_ids)} transects using lookback={best_lookback} ...\")\n",
    "\n",
    "# indices used repeatedly\n",
    "slr_pos = SEQUENCE_FEATURES.index(\"slr_mm\")\n",
    "ix_swh_storm = SEQUENCE_FEATURES.index(\"swh_storm_ratio\")\n",
    "ix_attack_sin = SEQUENCE_FEATURES.index(\"attack_sin\")\n",
    "ix_attack_cos = SEQUENCE_FEATURES.index(\"attack_cos\")\n",
    "ix_slr_x_storm = SEQUENCE_FEATURES.index(\"slr_x_storm\")\n",
    "ix_slr_x_sin = SEQUENCE_FEATURES.index(\"slr_x_sin\")\n",
    "ix_slr_x_cos = SEQUENCE_FEATURES.index(\"slr_x_cos\")\n",
    "\n",
    "# Precompute sorted SLR years after 2025\n",
    "slr_years_sorted = sorted(int(y) for y in future_slr[\"end_year\"].unique() if int(y) > 2025)\n",
    "\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    for tid in transect_ids:\n",
    "        res = build_initial_window_for_lookback(tid, best_lookback)\n",
    "        if res is None:\n",
    "            continue\n",
    "        window, meta_row = res\n",
    "        current_window = window.copy()\n",
    "        # keep region and bearing from meta_row (if present)\n",
    "        region_val = meta_row.get(\"region\", None)\n",
    "        bearing_val = meta_row.get(\"bearing_deg\", None)\n",
    "\n",
    "        for step_year in slr_years_sorted:\n",
    "            # get transect-specific slr\n",
    "            try:\n",
    "                slr_val = slr_lookup.loc[(tid, step_year)]\n",
    "            except KeyError:\n",
    "                # skip missing\n",
    "                continue\n",
    "\n",
    "            # update last row's slr + interactions\n",
    "            updated_last = current_window[-1].copy()\n",
    "            updated_last[slr_pos] = slr_val\n",
    "            swh_ratio_val = updated_last[ix_swh_storm]\n",
    "            attack_sin_val = updated_last[ix_attack_sin]\n",
    "            attack_cos_val = updated_last[ix_attack_cos]\n",
    "            updated_last[ix_slr_x_storm] = slr_val * swh_ratio_val\n",
    "            updated_last[ix_slr_x_sin] = slr_val * attack_sin_val\n",
    "            updated_last[ix_slr_x_cos] = slr_val * attack_cos_val\n",
    "\n",
    "            temp_window = current_window.copy()\n",
    "            temp_window[-1] = updated_last\n",
    "\n",
    "            # scale & predict\n",
    "            X_in = scale_window_with_saved_scaler(temp_window)   # shape (1, lookback, n_features)\n",
    "            X_tensor = torch.tensor(X_in, dtype=torch.float32).to(DEVICE)\n",
    "            pred_nsm = float(best_model(X_tensor).cpu().numpy().item())\n",
    "\n",
    "            # if this step_year is requested, store prediction\n",
    "            if step_year in FUTURE_YEARS:\n",
    "                outputs_per_year[step_year].append({\n",
    "                    \"transect_id\": tid,\n",
    "                    \"predicted_nsm_RNN\": pred_nsm,\n",
    "                    \"scenario_year\": step_year,\n",
    "                    \"region\": region_val,\n",
    "                    \"bearing_deg\": bearing_val\n",
    "                })\n",
    "\n",
    "# ---------------- SAVE PER-YEAR CSVs (with region + bearing_deg included) ----------------\n",
    "for year in FUTURE_YEARS:\n",
    "    rows = outputs_per_year.get(year, [])\n",
    "    if len(rows) == 0:\n",
    "        print(f\"No predictions for {year} — check FUTURE_SLR_FILE or historical depth.\")\n",
    "        continue\n",
    "    out_df = pd.DataFrame(rows)\n",
    "    cols = [\"region\", \"transect_id\", \"bearing_deg\", \"scenario_year\", \"predicted_nsm_RNN\"]\n",
    "    for c in cols:\n",
    "        if c not in out_df.columns:\n",
    "            out_df[c] = None\n",
    "    out_df = out_df[cols]\n",
    "    out_path = os.path.join(OUTDIR, f\"predictions_rnn_{year}.csv\")\n",
    "    out_df.to_csv(out_path, index=False)\n",
    "    print(\"Saved predictions:\", out_path)\n",
    "\n",
    "print(\"\\n✅ Done. Best model saved and recursive predictions exported.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
